"""
autonomous_agent.py: Unified Royal Diadem Backend—explicitly integrates 'silver', 'gold', 'cinnabar', 'combined' (decision/logic and logging)
Each action is logged to 'backend_core_service/logs/autonomy_log.log' with explicit tags and rationales.
"""

import os
import logging
from datetime import datetime
import subprocess
from backend.cinnabar.base import LLMClient
from backend.core.core_agent import symbolic_state


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
AUTONOMY_LOG = os.path.abspath(os.path.join(BASE_DIR, "../../logs/autonomy_log.log"))
CMD_LOG = os.path.abspath(os.path.join(BASE_DIR, "../../logs/cmd_execution.log"))

logger = logging.getLogger(__name__)


# ---- Explicit Logging Utils ----
def log_autonomy(message: str, level: str = "INFO"):
    dt = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{dt}] [AUTONOMY|{level}] {message}\n"
    try:
        with open(AUTONOMY_LOG, "a", encoding="utf-8") as f:
            f.write(line)
    except Exception as exc:
        logger.warning(f"Could not write to autonomy_log.log: {exc}")
    logger.info(line.strip())

def log_cmd(command, output, error, returncode):
    dt = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")
    try:
        with open(CMD_LOG, "a", encoding="utf-8") as f:
            f.write(
                f"[{dt}] [AUTONOMY|CMD] EXECUTE: {command}\n"
                f"[{dt}] [AUTONOMY|CMD] OUTPUT: {output}\n"
                f"[{dt}] [AUTONOMY|CMD] ERROR: {error}\n"
                f"[{dt}] [AUTONOMY|CMD] RETURNCODE: {returncode}\n"
            )
    except Exception as exc:
        logger.warning(f"Could not write to cmd_execution.log: {exc}")

def run_cmd(command_str):
    """Safely execute shell command and log it. Returns output string or error."""
    cmd = command_str if isinstance(command_str, list) else command_str.strip()
    log_autonomy(f"CMD Execution requested: {cmd}")
    # Recommend shell-interpreter parsing only if genuinely required
    try:
        # Prefer splits/simple parsing if command_str is str
        if isinstance(cmd, str):
            # Use shlex.split for real security, but here basic split
            import shlex
            cmd = shlex.split(cmd)
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        output = (result.stdout or "").strip()
        error = (result.stderr or "").strip()
        log_cmd(" ".join(cmd), output, error, result.returncode)
        if result.returncode == 0:
            return output
        else:
            return f"[CMD ERROR]\nERROR: {error}\nOUTPUT: {output}\nRC: {result.returncode}"
    except Exception as e:
        log_cmd(command_str, "", f"Exception: {str(e)}", -999)
        return f"[CMD EXCEPTION] {e}"


class AutonomousAgent:
    """
    Unified Royal Diadem Autonomous Agent (backend)
    - Explicitly integrates Silver (CMD), Gold (Codex/code-gen), Cinnabar (NLU/Response), Combined (decision logic)
    - Each step is logged with clear tags: SILVER (CMD), GOLD (CODEX), CINNABAR (NLU/assistant), COMBINED (decision & fallback)
    - Tool selection and result, as well as all actions, are logged to autonomy_log.log
    """
    def __init__(self, system_prompt: str = "You are an explicit autonomous Python agent."):
        self.llm = LLMClient(
            system_prompt=system_prompt,
            model="gpt-4",
            max_tokens=1024,
            temperature=0.5,
            symbolic_state=symbolic_state,
        )
        self.state = symbolic_state

    def reason_decide_act(self, user_input: str = None, **kwargs):
        """
        Accept text command, reason explicitly—using unified Gold/Silver/Cinnabar/Combined logic:
        - If CMD-suitable: run as Silver path (shell command)
        - If code gen/edit: run as Gold path (Codex/codex_auto)
        - Otherwise: Cinnabar (LLM-response/assistant)
        All steps (input, decision, result) are explicitly logged.
        """
        log = []
        log.append(f"Received user input: {user_input}")
        log_autonomy(f"[COMBINED] Received input: {user_input}")

        # --- Step 1: Validate input ---
        if not user_input or not user_input.strip():
            log.append("Input empty: rejecting as invalid.")
            log_autonomy("[COMBINED] Rejected as invalid: input empty.", level="WARNING")
            self.state.update("autonomous_agent_last_reasoning", log)
            return {"log": log, "decision": "invalid", "result": None, "symbolic_state": self.state.get_state()}

        # --- Step 2: Gold/Silver/Cinnabar/Combined: explicit tool-auto-selection
        # This classifier prompt can be easily extended
        decision_prompt = (
            "Classify this request as one and only one explicit path: \n"
            "SILVER: Run a shell/system command to control the OS or files.\n"
            "GOLD: Generate, edit, or refactor code (Codex/codex_auto).\n"
            "CINNABAR: Interpret/respond as pure LLM/assistant.\n"
            f"User Request: {user_input}\nType (SILVER/GOLD/CINNABAR):"
        )
        decision_type = self.llm.chat(decision_prompt).strip().upper()
        log.append(f"[COMBINED] LLM tool decision: {decision_type}")
        log_autonomy(f"[COMBINED] LLM tool decision for '{user_input}': {decision_type}")

        # --- Step 3: Dispatch to selected tool ---
        result = None
        if "SILVER" in decision_type or "CMD" in decision_type:
            # -- SILVER: OS automation via CMD
            log.append(f"[SILVER] Tool selected: CMD. Invoking '{user_input}'")
            log_autonomy(f"[SILVER] CMD execution starting: {user_input}")
            result = run_cmd(user_input)
            log_autonomy(f"[SILVER] CMD execution done. Result: {result[:200]}", level="RESULT")
        elif "GOLD" in decision_type or "CODEX" in decision_type:
            # -- GOLD: code generation/code editing
            log.append(f"[GOLD] Tool selected: CODEX CLI (code-gen)")
            log_autonomy("[GOLD] Using Codex/Codex-auto for code generation.")
            from backend.core import codex_auto
            code_result = codex_auto.generate_script(user_input)
            result = f"[GOLD][CODEX OUTPUT]\n{code_result}"
            log_autonomy(f"[GOLD] CODEX result: {code_result[:200]}", level="RESULT")
        else:
            # -- CINNABAR: LLM/assistant interpretation
            log.append(f"[CINNABAR] Tool selected: LLM/Assistant/Response.")
            log_autonomy("[CINNABAR] LLM fallback (responding as symbolic/assistant)")
            symbolic_result = self.llm.chat(user_input)
            result = f"[CINNABAR][LLM RESULT]\n{symbolic_result}"
            log_autonomy(f"[CINNABAR] LLM result: {symbolic_result[:200]}", level="RESULT")

        # --- Step 4: Final logging, symbolic update ---
        log.append(f"Outcome: {result}")
        log_autonomy(f"[COMBINED] Outcome: {result[:120]}", level="OUTCOME")
        self.state.update("autonomous_agent_last_reasoning", log)
        self.state.update("autonomous_agent_last_decision", decision_type)
        self.state.update("autonomous_agent_last_result", result)
        return {
            "log": log,
            "decision": decision_type,
            "result": result,
            "symbolic_state": self.state.get_state(),
        }

    @staticmethod
    def process_chat(chat: str):
        """
        Unified Chat API: Accept user text, reason via explicit unified tool-logic,
        select SILVER (CMD), GOLD (Codex), or CINNABAR (LLM), act, and log each step to autonomy_log.log
        The action log includes clear tool tags and outcomes for transparency.
        """
        agent = AutonomousAgent(system_prompt="You are Sonny, the unified backend Royal Diadem agent. For every step, log your explicit tool choices and their result. Clearly state if you pick SILVER, GOLD, or CINNABAR.")
        output = agent.reason_decide_act(chat)
        response = "\n".join([f"[log] {step}" for step in output["log"]])
        response += f"\n\n[Sonny Action: {output['decision']}]\n{output['result']}"
        return response